# Learning to Steer: Input-dependent Steering for Multimodal LLMs

<div align="left">
<h3>
NeurIPS 2025
</h3>

<a href="https://jayneelparekh.github.io/learn-to-steer/"> <img alt="Blog Post" src="https://img.shields.io/badge/L2S-Project/Blog Page-911b91"> </a> [![arXiv](https://img.shields.io/badge/arXiv-2508.12815-b31b1b.svg)](https://arxiv.org/abs/2508.12815)

## Abstract/Overview


## Installation

This repository is built on top of our [XL-VLMs](https://github.com/mshukor/xl-vlms) repository which contains code for our previous works on explainability and steering of multimodal LLMs. Feel free to refer to explore it if these works are relevant for you.

### Environment 

### Datasets

### Models

We support models from the `transformers` library. Our current experiments are on the following models:
* **LLaVA-v1.5-7b**
* **Qwen2-VL-7B-Instruct**

## Main Experiments

### Feature Extraction

### Training L2S

### Inference 

### Evaluation




## Citations

If you find this repo useful, you can cite the work as follows:

```bibtex
@article{parekh2025learning,
  title={Learning to Steer: Input-dependent Steering for Multimodal LLMs},
  author={Parekh, Jayneel and Khayatan, Pegah and Shukor, Mustafa and Dapogny, Arnaud and Newson, Alasdair and Cord, Matthieu},
  journal={Advances in Neural Information Processing Systems},
  year={2025}
}
```
