<!DOCTYPE html>
<html lang="en" class="fontawesome-i2svg-active fontawesome-i2svg-complete">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <script type="text/javascript"
  src="https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js">
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Learning to Steer: Input-dependent Steering for Multimodal LLMs</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6">
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
      <script defer="" src="css/fontawesome.all.min.js"></script>
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <style>
      body {
        font-size:16px;
        margin:80px auto;
        width:auto;
        max-width:850px;
      }

      hr {
        border:0;
        height:1.0px;
        background-image:linear-gradient(to right, rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3));
      }

      .gap-30 {
      width:100%;
      height:30px;
      }

      .gap-20 {
      width:100%;
      height:20px;
      }

      .gap-10 {
      width:100%;
      height:10px;
      }

      .gap-5 {
      width:100%;
      height:5px;
      }
      .paper {
        max-width: 700px;
      }
      @media (max-width: 910px) {
        .paper {
          max-width: 500px;
        }
      }
      @media (max-width: 610px) {
        .paper {
          max-width: 300px;
        }
      }
    </style>
  </head>
  
  <body>

    <center><span style="font-size:40px; color:rgb(75, 15, 101)">
        <b> Learning to Steer: Input-dependent Steering <br> for  Multimodal LLMs </b>
    </span></center>
    <div class="gap-20"></div>

    <!---------------------  authors --------------------->
    <div class="row">
        &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp;
        <p>
          <a href="https://jayneelparekh.github.io/"><span style="font-size:24px; color:rgb(172, 138, 30)">Jayneel Parekh*</span></a>
          &emsp; &emsp;
          <a href="https://pegah-kh.github.io/"><span style="font-size:24px; color:rgb(172, 138, 30)">Pegah Khayatan*</span></a>
          &emsp; &emsp;
          <a href="https://mustafashukor.github.io/"><span style="font-size:24px; color:rgb(172, 138, 30)">Mustafa Shukor</span></a>
          &emsp; &emsp;
          <br>
          <a href="https://www.linkedin.com/in/arnaud-dapogny-12653493/"><span style="font-size:24px; color:rgb(172, 138, 30)">Arnaud Dapogny</span></a>
          &emsp; &nbsp;
          <a href="https://sites.google.com/site/alasdairnewson/"><span style="font-size:24px; color:rgb(172, 138, 30)">Alasdair Newson</span></a>
          &emsp; &emsp; &nbsp;
          <a href="https://cord.isir.upmc.fr/"><span style="font-size:24px; color:rgb(172, 138, 30)">Matthieu Cord</span></a>
        </p>
    </div>
    
    <div class="row">
        &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp;  &emsp;
        <p style="font-size:24px; color:rgb(172, 138, 30)">ISIR, Sorbonne Universit√©, France </p>
    </div>

     <div class="gap-10"></div>

    <center>
        <span style="font-size:21px">
          [<a href="https://arxiv.org/abs/2508.12815" style="color:rgb(75, 15, 101)">Paper</a>]
          &emsp; 
          [<a href="https://github.com/mshukor/xl-vlms" style="color:rgb(75, 15, 101)">Code</a>]&nbsp;
        </span>
    </center>
    
    <div class="gap-20"></div>

    <hr>
    <div class="gap-10"></div>

    <b><span style="font-size:36px; color:rgb(75, 15, 101)">Key Takeaways</span></b>
    <div class="gap-10"></div>
    
    <ul style="font-size:19px">
      <li> This work is on steering for multimodal LLMs (MLLMs). The central theme in our work is that a steering behavior can instantiate itself differently for different inputs. This motivates the need to steer differently depending upon input context.</li>
      <div class="gap-10"></div>
      <li> We first propose <a href="#p2s" style="color:rgb(172, 138, 30)"><b><u>Prompt-to-Steer (P2S)</u></b></a>, a type of contrastive steering method which demonstrates that if the behavior instantiation is known in advance, one can use input specific contrastive prompts to extract and steer the given input directly.</li>
      <div class="gap-10"></div>
      <li> Since P2S requires knowing the instantiation in advance it is not directly useful in practice. We then propose <a href="#l2s" style="color:rgb(172, 138, 30)"><b><u>Learn-to-Steer (L2S)</u></b></a> method that learns to predict input-specific P2S steering vectors from input context. The L2S prediction is used to steer the MLLM.</li>
      <div class="gap-10"></div>
      <li> We apply L2S to steer MLLMs for two different applications: safety enforcement and hallucination mitigation. Compared to the popular mean-steering approach, L2S allows for simultaneous steering across multiple behavior instantiations.</li>
    </ul>


    <div class="gap-20"></div>

    <hr>

    <b><span style="font-size:36px; color:rgb(75, 15, 101)"> 
      Background and Motivation
    </span></b>

    <div class="gap-10"></div>

    <p style="font-size:19px">
    <i>Representation steering</i> refers to the idea of intervening on the internal representations in order to "align" the output with certain desired behaviors. 
       For computer vision enthusiasts, in principle, it's similar to a class of image editing approaches where one intervenes on the latent representations of image generative models to control the output "style".
    </p>

    <p style="font-size:19px">
    For a given desired behavior (eg. sycophancy), most current steering approaches involve contrasting representations between two sets of samples, one that aligns with the given behavior, and the other that doesn't. 
    Subsequently, one extracts mean-of-difference or difference-of-means between the two sets of representations as a single steering vector to induce desired behavior. 
    </p>

    <p style="font-size:19px">
    The core of our proposal is that when steering for a given goal/behavior (eg. ensuring safety), it can realize itself differently for different inputs. 
    For example, OpenAI usage policy provides a set of unsafe scenarios where a model response should not be used/relied upon. 
    One could steer so that the model refuses a user query regardless of the scenario, which would ensure a safe response. However, safety can mean different things depending upon input context.  
    It would entail refusal for queries asking about performing harmful/illegal activities. 
    But for legal/financial/healthcare queries, what is arguably more essential is not refusal but that response should recommend (atleast at some point) to defer the user to consult a human expert (lawyer/doctor etc.)
    This can be regarded as a subjective opinion about desired model behave, and that is precisely the point!
    This motivated us to propose a method that can steer differently, according to input context. 
    </p>

    <div class="gap-20"></div>

    <hr>

    <b><span style="font-size:36px; color:rgb(75, 15, 101)"> 
      Method
    </span></b>

    <center>
    <video width="810" height="480" autoplay loop muted>
    <source src="assets/system_design_recording_v3.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video>
    </center>

    <div id="p2s">
    <b><span style="font-size:24px; color:rgb(172, 138, 30)"> 
    Prompt-to-Steer (P2S)
    </span></b>
    </div>
   
    <br>

    <p style="font-size:19px">
    For a given multimodal input query \(X=(I, T) \), we define a pair of contrastive prompts, \( (T^+_X, T^-_X) \), that correspond to desired and undesired behaviors respectively. 
    Crucially, unlike previous steering methods, that use a fixed set of prompts for all samples, we allow use of input-specific prompts, aligned with desired behavior instantiation for the given input. 
    These prompt completions are appended to original input to create modified inputs \(X^+ = (I, T || T^+_X)\), \(X^- = (I, T || T^-_X)\). 
    The difference between the final (generally) token residual stream representations is extracted at the steering layer \(L^*\) as the input-specific steering vector \(z_{X, L^*} \in \mathbb{R}^D\).

    To steer the given input one can simply apply \(z_{X, L^*}\) to shift the hidden representations at layer \(L^*\).
    </p>

    <center style="font-size:19px">
      \(h_{L^*}^p(X) \leftarrow h_{L^*}^p(X) + \alpha z_{X, L^*}\)
    </center>

    <br>
    <p style="font-size:19px">
    Importantly, <b>if we know the behavior instantiation in advance</b>, P2S uses only the sample and input-specific prompt completions to extract a steering vector the given input. 
    Examples of such prompt completions are illustrated below
    </p>

    <center>
      <embed src="assets/contrastive_prompts2_harmful.png" width="700px" height="200px" />
      <embed src="assets/contrastive_prompts2_healthcare.png" width="700px" height="210px" />
    </center>

     <br>

    <div id="l2s">
    <b><span style="font-size:24px; color:rgb(172, 138, 30)"> 
    Learn-to-Steer (L2S)
    </span></b>
    </div>
   
    <br>

    <p style="font-size:19px">
      Since P2S requires knowing behavior instantiation in advance, to determine appropriate prompt completions \( (T^+_X, T^-_X) \), it is not particularly useful in practice. 
      To address this limitation we propose Learn-to-Steer (L2S) that learns a model \(g_{\Theta}: \mathbb{R}^D \rightarrow \mathbb{R}^D \) to predict the P2S steering vectors from the input context vector \(h_{X, L'}\) (last token of input at layer \(L'\)).
    </p>

    <center style="font-size:19px">
      \( \Theta^* = \texttt{argmin}_\Theta \ \mathbb{E}_X [\|z_{X,L^*} - g_\Theta(h_{X, L'})\|^2_2], \quad  h_{L^*}^p \leftarrow h_{L^*}^p + \alpha g_{\Theta^*}(h_{X, L'}) \)
    </center>

    <br>

    <p style="font-size:19px">
      During inference, one can simply use \(g_{\Theta^*}\) to directly predict the appropriate steering vector from the input context. In our experiments \(g_{\Theta}\) is modeled as a two layer MLP with Tanh or ReLU activation.
      It is worth noting that training \(g\) is highly time and memory efficient since one directly operates in the latent space. It requires the original model \(f\) only to extract the representations via a forward pass.
      The original model is not needed during its training. In this sense, it preserves the cost and ease of application advantage of standard steering methods while offering more flexibility in steering behavior one can enforce.
      For complete method details please refer to the main paper and appendix.
    </p>

    <div class="gap-20"></div>

    <hr>

    <b><span style="font-size:36px; color:rgb(75, 15, 101)"> 
      Experiments
    </span></b>

    <div class="gap-10"></div>

    <p style="font-size:19px">
      We test our method for safety enforcement and hallucination mitigation tasks on LLaVA-v1.5. 
      The safety enforcement experiments are conducted on MMSafetyBench dataset, while hallucination experimnents are on primarily on POPE dataset.
      Additional experiments with Qwen2-VL-7B-Instruct on both benchmarks, and LLaVA-v1.5 on VLGuard dataset for safety enforcement will be added soon in updated version of our paper. 
    </p>


  </body>
  
</html>
