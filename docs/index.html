<!DOCTYPE html>
<html lang="en" class="fontawesome-i2svg-active fontawesome-i2svg-complete">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <script type="text/javascript"
  src="https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js">
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Learning to Steer: Input-dependent Steering for Multimodal LLMs</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6">
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
      <script defer="" src="css/fontawesome.all.min.js"></script>
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <style>
      body {
        font-size:16px;
        margin:80px auto;
        width:auto;
        max-width:850px;
      }

      hr {
        border:0;
        height:1.0px;
        background-image:linear-gradient(to right, rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3));
      }

      .gap-30 {
      width:100%;
      height:30px;
      }

      .gap-20 {
      width:100%;
      height:20px;
      }

      .gap-10 {
      width:100%;
      height:10px;
      }

      .gap-5 {
      width:100%;
      height:5px;
      }
      .paper {
        max-width: 700px;
      }
      @media (max-width: 910px) {
        .paper {
          max-width: 500px;
        }
      }
      @media (max-width: 610px) {
        .paper {
          max-width: 300px;
        }
      }

      .ref-link {
    position: relative;
    text-decoration: none;
    }
  
    .ref-link .tooltip {
    visibility: hidden;
    width: 250px;
    background-color: #555;
    color: #b49347;
    text-align: left;
    border-radius: 6px;
    padding: 10px;
    position: absolute;
    z-index: 1;
    bottom: 125%;
    left: 50%;
    margin-left: -125px;
    opacity: 0;
    transition: opacity 0.3s;
    font-size: 0.85em;
    }
  
    .ref-link:hover .tooltip {
    visibility: visible;
    opacity: 1;
    }
    </style>
  </head>
  
  <body>

    <center><span style="font-size:40px; color:rgb(75, 15, 101)">
        <b> Learning to Steer: Input-dependent Steering <br> for  Multimodal LLMs </b>
    </span></center>
    <div class="gap-20"></div>

    <!---------------------  authors --------------------->
    <div class="row">
        &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp;
        <p>
          <a href="https://jayneelparekh.github.io/"><span style="font-size:24px; color:rgb(172, 138, 30)">Jayneel Parekh*</span></a>
          &emsp; &emsp;
          <a href="https://pegah-kh.github.io/"><span style="font-size:24px; color:rgb(172, 138, 30)">Pegah Khayatan*</span></a>
          &emsp; &emsp;
          <a href="https://mustafashukor.github.io/"><span style="font-size:24px; color:rgb(172, 138, 30)">Mustafa Shukor</span></a>
          &emsp; &emsp;
          <br>
          <a href="https://www.linkedin.com/in/arnaud-dapogny-12653493/"><span style="font-size:24px; color:rgb(172, 138, 30)">Arnaud Dapogny</span></a>
          &emsp; &nbsp;
          <a href="https://sites.google.com/site/alasdairnewson/"><span style="font-size:24px; color:rgb(172, 138, 30)">Alasdair Newson</span></a>
          &emsp; &emsp; &nbsp;
          <a href="https://cord.isir.upmc.fr/"><span style="font-size:24px; color:rgb(172, 138, 30)">Matthieu Cord</span></a>
        </p>
    </div>
    
    <div class="row">
        &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp;  &emsp;
        <p style="font-size:24px; color:rgb(172, 138, 30)">ISIR, Sorbonne Université, France </p>
    </div>

     <div class="gap-10"></div>

    <center>
        <span style="font-size:21px">
          [<a href="https://arxiv.org/abs/2508.12815" style="color:rgb(75, 15, 101)">Paper</a>]
          &emsp; 
          [<a href="https://github.com/jayneelparekh/learn-to-steer" style="color:rgb(75, 15, 101)">Code</a>]&nbsp;
        </span>
    </center>
    
    <div class="gap-20"></div>

    <hr>
    <div class="gap-10"></div>

    <b><span style="font-size:36px; color:rgb(75, 15, 101)">Key Takeaways</span></b>
    <div class="gap-10"></div>
    
    <ul style="font-size:19px">
      <li> This work is on steering for <b>Multimodal LLMs (MLLMs)</b>. The central theme in our work is that a steering behavior can instantiate itself differently for different inputs. This motivates the need to steer in different directions depending upon input context.</li>
      <div class="gap-10"></div>
      <li> We first propose <a href="#p2s" style="color:rgb(172, 138, 30)"><b><u>Prompt-to-Steer (P2S)</u></b></a>, a type of contrastive steering method which demonstrates that if the behavior instantiation is known in advance, one can use input specific contrastive prompts to extract and steer the given input directly.</li>
      <div class="gap-10"></div>
      <li> Since P2S requires knowing the instantiation in advance it is not directly useful in practice. We then propose <a href="#l2s" style="color:rgb(172, 138, 30)"><b><u>Learn-to-Steer (L2S)</u></b></a> method that learns to predict input-specific P2S steering vectors from input context. The L2S prediction is used to steer the MLLM.</li>
      <div class="gap-10"></div>
      <li> We apply L2S to steer MLLMs for two different applications: <a href="#safety_exps" style="color:rgb(172, 138, 30)">safety enforcement</a> and <a href="#hallucination_exps" style="color:rgb(172, 138, 30)">hallucination mitigation</a>. Compared to the popular mean-steering approach, L2S allows for simultaneous steering across multiple behavior instantiations.</li>
    </ul>

    <br>

    <center>
      <embed src="assets/intro_teaser.png" width="870px" height="400px" />
    </center>



    <div class="gap-20"></div>
    <div class="gap-20"></div>

    <hr>

    <div class="gap-20"></div>

    <b><span style="font-size:36px; color:rgb(75, 15, 101)"> 
      Background and Motivation
    </span></b>

    <div class="gap-10"></div>

    <p style="font-size:19px">
    <i>Representation steering</i> refers to the idea of intervening on the internal representations in order to "align" the output with certain desired behaviors. 
       For computer vision enthusiasts, in principle, it's similar to a class of image editing approaches where one intervenes on the latent representations of image generative models to control the output "style" <sup><a href="#ref1" id="cite1" class="ref-link">[1, 2]</a></sup>.
    </p>

    <p style="font-size:19px">
    For a given desired behavior (eg. sycophancy), most current steering approaches involve contrasting representations between two sets of samples, one that aligns with the given behavior, and the other that doesn't. 
    Subsequently, one extracts mean-of-difference or difference-of-means between the two sets of representations as a single steering vector to induce desired behavior <sup><a href="#ref3" id="cite3" class="ref-link">[3-7]</a></sup>. Some cool recent works apply a steering vector conditionally <sup><a href="#ref8" id="cite8" class="ref-link">[8]</a></sup> or adapt the steering strength to incorporate abstention <sup><a href="#ref15" id="cite15" class="ref-link">[15]</a></sup>. Nevertheless, the direction of steering does not change with input.
    </p>

    <p style="font-size:19px">
    The core of our proposal is that when steering for a given goal/behavior (eg. ensuring safety), it can realize itself differently for different inputs. 
    For example, OpenAI usage policy provides a set of unsafe scenarios where a model response should not be used/relied upon. 
    One could steer so that the model refuses a user query regardless of the scenario, which would ensure a safe response. However, safety can mean different things depending upon input context.  
    It would entail refusal for queries asking about performing harmful/illegal activities. 
    But for legal/financial/healthcare queries, what is arguably more essential is not refusal but that response should recommend (atleast at some point) to defer the user to consult a human expert (lawyer/doctor etc.)
    This can be regarded as a subjective opinion about desired model behavior, and that is precisely the point! The desired behavior varies according to user preferences and can also reflect for type of input at hand. 
    This motivated us to propose a method that for a given goal, can flexibly steer in different directions, according to the input context. 
    </p>

    <div class="gap-20"></div>

    <hr>

    <b><span style="font-size:36px; color:rgb(75, 15, 101)"> 
      Method
    </span></b>

    <center>
    <video width="810" height="480" autoplay loop muted>
    <source src="assets/system_design_recording_v3.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video>
    </center>

    <div id="p2s">
    <b><span style="font-size:24px; color:rgb(172, 138, 30)"> 
    Prompt-to-Steer (P2S)
    </span></b>
    </div>
   
    <br>

    <p style="font-size:19px">
    For a given multimodal input query \(X=(I, T) \), we define a pair of contrastive prompts, \( (T^+_X, T^-_X) \), that correspond to desired and undesired behaviors respectively. 
    Crucially, unlike previous steering methods, that use a fixed set of prompts for all samples, we allow use of input-specific prompts, aligned with desired behavior instantiation for the given input. 
    These prompt completions are appended to original input to create modified inputs \(X^+ = (I, T || T^+_X)\), \(X^- = (I, T || T^-_X)\). 
    The difference between the final (generally) token residual stream representations is extracted at the steering layer \(L^*\) as the input-specific steering vector \(z_{X, L^*} \in \mathbb{R}^D\).

    To steer the given input one can simply apply \(z_{X, L^*}\) to shift the hidden representations at layer \(L^*\).
    </p>

    <center style="font-size:19px">
      \(h_{L^*}^p(X) \leftarrow h_{L^*}^p(X) + \alpha z_{X, L^*}\)
    </center>

    <br>
    <p style="font-size:19px">
    Importantly, <b>if we know the behavior instantiation in advance</b>, P2S uses only the sample and input-specific prompt completions to extract a steering vector the given input. 
    Examples of such prompt completions are illustrated below
    </p>

    <center>
      <embed src="assets/contrastive_prompts2_harmful.png" width="700px" height="200px" />
      <embed src="assets/contrastive_prompts2_healthcare.png" width="700px" height="210px" />
    </center>

     <br>

    <div id="l2s">
    <b><span style="font-size:24px; color:rgb(172, 138, 30)"> 
    Learn-to-Steer (L2S)
    </span></b>
    </div>
   
    <br>

    <p style="font-size:19px">
      Since P2S requires knowing behavior instantiation in advance, to determine appropriate prompt completions \( (T^+_X, T^-_X) \), it is not particularly useful in practice. 
      To address this limitation we propose Learn-to-Steer (L2S) that learns a model \(g_{\Theta}: \mathbb{R}^D \rightarrow \mathbb{R}^D \) to predict the P2S steering vectors from the input context vector \(h_{X, L'}\) (last token of input at layer \(L'\)).
    </p>

    <center style="font-size:19px">
      \( \Theta^* = \texttt{argmin}_\Theta \ \mathbb{E}_X [\|z_{X,L^*} - g_\Theta(h_{X, L'})\|^2_2], \quad  h_{L^*}^p \leftarrow h_{L^*}^p + \alpha g_{\Theta^*}(h_{X, L'}) \)
    </center>

    <br>

    <p style="font-size:19px">
      During inference, one can simply use \(g_{\Theta^*}\) to directly predict the appropriate steering vector from the input context. In our experiments \(g_{\Theta}\) is modeled as a two layer MLP with Tanh or ReLU activation.
      It is worth noting that training \(g\) is highly time and memory efficient since one directly operates in the latent space. It requires the original model \(f\) only to extract the representations via a forward pass.
      The original model is not needed during its training. In this sense, it preserves the cost and ease of application advantage of standard steering methods while offering more flexibility in steering behavior one can enforce.
      For complete method details please refer to the main paper and appendix.
    </p>

    <div class="gap-20"></div>

    <hr>

    <b><span style="font-size:36px; color:rgb(75, 15, 101)"> 
      Experiments
    </span></b>

    <div class="gap-10"></div>

    <p style="font-size:19px">
      We test our method for safety enforcement and hallucination mitigation tasks on LLaVA-v1.5 <sup><a href="#ref9" id="cite9" class="ref-link">[9]</a></sup> and Qwen2-VL-7B-Instruct <sup><a href="#ref10" id="cite10" class="ref-link">[10]</a></sup>. 
      The safety enforcement experiments are conducted on MMSafetyBench (primary) <sup><a href="#ref11" id="cite11" class="ref-link">[11]</a></sup> and VLGuard <sup><a href="#ref12" id="cite12" class="ref-link">[12]</a></sup> datasets, while hallucination experimnents are on primarily on POPE (primary) <sup><a href="#ref13" id="cite13" class="ref-link">[13]</a></sup>, COCO <sup><a href="#ref14" id="cite14" class="ref-link">[14]</a></sup> datasets. 
      Please refer to the paper for further details. 
    </p>

    <p style="font-size:19px">
      <b>Baselines:</b> The primary baseline for <b>L2S</b>, <b>P2S</b> methods is the mean-steering (<b>Mean-S</b>) method that uses \(\mathbb{E}(z_{X, L^*})\) as the fixed steering vector. 
      As before, for further info about other baselines and more precise details, please refer to the paper.
    </p>

    <div class="gap-10"></div>

    <div id="safety_exps">
    <b><span style="font-size:24px; color:rgb(172, 138, 30)"> 
    Inducing Safety
    </span></b>
    </div>

    <div class="gap-10"></div>


    <p style="font-size:19px">
      Our safety steering goal here is to avoid outputting any unsafe instructions for any query (about harmful/illegal activities or otherwise), 
      while being able to defer the user to a human expert for any legal/financial/healthcare queries (which use different prompt completions). 
    </p>

    <p style="font-size:19px">
      Quantiative experiments can be found in the paper, where the main takeaway is that L2S is able to much better at steering simultaneously for both kind of behavior instantiations depending on the input context. 
      Some qualitative examples illustrating this are added below:
    </p>

    <center>
      <embed src="assets/qualitative_app_safety_2-1.png" width="880px" height="450px" />
      <br> <br> 
      <embed src="assets/qualitative_app_safety_3-1.png" width="880px" height="450px" />
      <br> <br>
      <embed src="assets/qualitative_app_ed_1-1.png" width="880px" height="450px" />
    </center>

    <!-- 

    <p style="font-size:19px">
      <b>Metrics:</b> We use three metrics to evaluate the steered responses 
      <ul style="font-size:18px">
        <li> <b>Harmfulness evaluation:</b> For each steered test response, we compute a probability of it being unsafe/harmful using Llama-Guard-3-8B. We threshold these probabilities to compute the \( \textrm{Unsafe-score}(p) \) that quantifies fraction of responses with harmfulness greater than threshold \(p \in [0, 1]\). </li> 
        <li> <b>Expert-deference score:</b> This evaluates fraction of test responses for the three scenarios where the model defers the user to an expert at some point in its output</li>
        <li> <b>Response Quality:</b> This evaluates (using Gemini-2.0-Flash) how well the steered responses remain coherent and relevant to the context of the given image. This metric acts more like a control axis for all steering methods.</li>
      </ul>
    </p>

    <center>
      <embed src="assets/contrastive_prompts2_harmful.png" width="700px" height="200px" />
      <embed src="assets/contrastive_prompts2_healthcare.png" width="700px" height="210px" />
    </center>
     -->


    <div class="gap-20"></div>

    <div id="hallucination_exps">
    <b><span style="font-size:24px; color:rgb(172, 138, 30)"> 
    Mitigating Hallucinations
    </span></b>
    </div>

    <div class="gap-10"></div>

    <p style="font-size:19px">
      We also evaluate our method on POPE dataset for hallucination mitigation. The dataset consists of multimodal queries asking about presence of various objects in a context image.
      The steering goal is to steer the response towards the correct answer (Yes/No) based on the input context. 
      Note that behavior instantiation in this case is implicit and inherent in the task itself. Quantiative results (in main paper) show L2S is indeed able to reduce hallucinations to some extent (while mean-steering fails).
      We show some qualitative examples below:
    </p>

    <center>
      <embed src="assets/pope_samples-1.png" width="800px" height="450px" />
      <br> <br> 
      <embed src="assets/skier_backpack-1.png" width="800px" height="450px" />
    </center>






<h2>References</h2>
<ol>
  <li id="ref1"> Kingma, D. P. and Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114. <a href="#cite1">↩</a></li>
  <li id="ref2"> Shen, Y., Yang, C., Tang, X., and Zhou, B. (2020). Interfacegan: Interpreting the disentangled face representation learned by gans. IEEE TPAMI. <a href="#cite1">↩</a></li>
  <li id="ref3"> Panickssery, N., Gabrieli, N., Schulz, J., Tong, M., Hubinger, E., and Turner, A. M. (2023). Steering llama 2 via contrastive activation addition. arXiv preprint arXiv:2312.06681. <a href="#cite3">↩</a></li>
  <li id="ref4"> Turner, A. M., Thiergart, L., Leech, G., Udell, D., Vazquez, J. J., Mini, U., and MacDiarmid, M. (2023). Activation addition: Steering language models without optimization. arXiv e-prints, pages arXiv–2308. <a href="#cite3">↩</a></li>
  <li id="ref5"> Arditi, A., Obeso, O., Syed, A., Paleka, D., Panickssery, N., Gurnee, W., and Nanda, N. (2024). Refusal in language models is mediated by a single direction. NeurIPS 2024. <a href="#cite3">↩</a></li>
  <li id="ref6"> Khayatan, P., Shukor, M., Parekh, J., Dapogny, A., and Cord, M. (2025). Analyzing fine-tuning representation shift for multimodal llms steering alignment. ICCV 2025. <a href="#cite3">↩</a></li>
  <li id="ref7"> Li, Q., Geng, J., Chen, Z., Song, K., Ma, L., and Karray, F. (2025). Internal activation revision: Safeguarding vision language models without parameter update. arXiv preprint arXiv:2501.16378 <a href="#cite3">↩</a></li>
  <li id="ref8"> Lee, B. W., Padhi, I., Ramamurthy, K. N., Miehling, E., Dognin, P., Nagireddy, M., and Dhurandhar, A. (2025). Programming refusal with conditional activation steering. ICLR 2025. <a href="#cite8">↩</a></li>
  <li id="ref9"> Liu, H., Li, C., Li, Y., and Lee, Y. J. (2024). Improved baselines with visual instruction tuning. CVPR 2024 <a href="#cite9">↩</a></li>
  <li id="ref10"> Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al. (2024b). Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution. arXiv preprint arXiv:2409.12191. <a href="#cite10">↩</a></li>
  <li id="ref11"> Liu, X., Zhu, Y., Gu, J., Lan, Y., Yang, C., and Qiao, Y. (2024b). Mm-safetybench: A benchmark for safety evaluation of multimodal large language models. ECCV 2024. <a href="#cite11">↩</a></li>
  <li id="ref12"> Zong, Y., Bohdal, O., Yu, T., Yang, Y., and Hospedales, T. (2024). Safety fine-tuning at (almost) no cost: A baseline for vision large language models. ICML 2024. <a href="#cite12">↩</a></li>
  <li id="ref13"> Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and rong Wen, J. (2023). Evaluating object hallucination in large vision-language models. EMNLP 2023 <a href="#cite13">↩</a></li>
  <li id="ref14"> Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P., and Zitnick, C. L. (2014). Microsoft coco: Common objects in context. ECCV 2014. <a href="#cite14">↩</a></li>
  <li id="ref15"> Hedström, A., Amoukou, S. I., Bewley, T., Mishra, S., and Veloso, M. (2025). To steer or not to steer? mechanistic error reduction with abstention for language models. ICML 2025. <a href="#cite15">↩</a></li>
</ol>



















  </body>
  
</html>
